{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"nvidiaL4","dataSources":[{"sourceId":86023,"databundleVersionId":9869096,"sourceType":"competition"},{"sourceId":205183965,"sourceType":"kernelVersion"},{"sourceId":180858,"sourceType":"modelInstanceVersion","modelInstanceId":154124,"modelId":176602},{"sourceId":181353,"sourceType":"modelInstanceVersion","modelInstanceId":154560,"modelId":172131}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":803.741372,"end_time":"2024-10-27T06:12:38.713054","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-27T05:59:14.971682","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"3ae3375ceefd42f9aeede66494e08172":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_712906b89a034aa981c78ec917695c16","IPY_MODEL_6e02449d72574cb59a7408135ddf0606","IPY_MODEL_aabe4ff8b2734c9c86a33deb7a3d0a52"],"layout":"IPY_MODEL_d865f8a6c163451da7719a6d6e720f96"}},"63273838b2cf407a9d8a4b3d3c0a6096":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64efbe72d2c24a98b72db29e311e7206":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e02449d72574cb59a7408135ddf0606":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_805d7686fa99450fa5f28d53a2e50938","max":11,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c76df798b0e0495abc6131f26e897eca","value":11}},"712906b89a034aa981c78ec917695c16":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af9c9877f8c347788ca34cdd8170fc94","placeholder":"â€‹","style":"IPY_MODEL_c9c855af5c4941d6bed6542ce711a012","value":""}},"805d7686fa99450fa5f28d53a2e50938":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aabe4ff8b2734c9c86a33deb7a3d0a52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63273838b2cf407a9d8a4b3d3c0a6096","placeholder":"â€‹","style":"IPY_MODEL_64efbe72d2c24a98b72db29e311e7206","value":"Loadingâ€‡safetensorsâ€‡checkpointâ€‡shards:â€‡100%â€‡Completedâ€‡|â€‡11/11â€‡[04:33&lt;00:00,â€‡26.87s/it]\n"}},"af9c9877f8c347788ca34cdd8170fc94":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c76df798b0e0495abc6131f26e897eca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c9c855af5c4941d6bed6542ce711a012":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d865f8a6c163451da7719a6d6e720f96":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"- It's a copy of  [https://www.kaggle.com/code/mbmmurad/lb-20-qwq-32b-preview-optimized-inference](https://www.kaggle.com/code/mbmmurad/lb-20-qwq-32b-preview-optimized-inference)  with tiny changes. \n\n- I changed the number of samples from 5 to 6 ( I tried 7 but no better than 6 ( not sure, just in my notebooks so far ) ). \n\n- The added one is designed to use TIR, but I don't think the improvement is mainly because of it, probably luck. But Increasing number of samples ( with some other prompts ) may be of use, I tried and get a little improvement. ","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"text-align: center; font-size: 2em; font-weight: bold; color: #2c3e50; margin-bottom: 0.5em;\">\n   More Ain't Always Better\n</h1>\n\n<h2 style=\"text-align: center; font-size: 1.5em; color: #34495e; font-style: italic; margin-top: 0;\">\n   QWQ-32B-Preview Optimized Inference\n</h2>\n\n<p style=\"font-size: 1.1em; color: #555;\">\n   This notebook is modified from \n   <a href=\"https://www.kaggle.com/code/boristown/qwen-qwq-32b-preview-deepreasoning\" target=\"_blank\" style=\"color: #007acc; text-decoration: none;\">\n       this fantastic notebook\n   </a> by \n   <a href=\"https://www.kaggle.com/boristown\" target=\"_blank\" style=\"color: #007acc; text-decoration: none;\">\n       @boristown\n   </a>, which was also inspired by \n   <a href=\"https://www.kaggle.com/code/huikang/qwen2-5-72b-instruct-with-tir\" target=\"_blank\" style=\"color: #007acc; text-decoration: none;\">\n       this great notebook\n   </a> by \n   <a href=\"https://www.kaggle.com/huikang\" target=\"_blank\" style=\"color: #007acc; text-decoration: none;\">\n       @huikang\n   </a> and \n   <a href=\"https://www.kaggle.com/code/konstantinboyko/qwen2-5-72b-instruct\" target=\"_blank\" style=\"color: #007acc; text-decoration: none;\">\n       this one\n   </a> by \n   <a href=\"https://www.kaggle.com/konstantinboyko\" target=\"_blank\" style=\"color: #007acc; text-decoration: none;\">\n       @konstantinboyko\n   </a>.\n</p>\n\n<p style=\"font-size: 1.1em;\">\n   Special thanks to Ali \n   <a href=\"https://www.kaggle.com/asalhi\" target=\"_blank\" style=\"color: #007acc; text-decoration: none;\">\n       @asalhi\n   </a> \n   for sharing insights about QwQ-32B-preview scores and uploading the AWQ version.\n</p>","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"text-align: center; font-size: 1.8em; color: #2c3e50;\">Optimizing the Public Notebook</h1>\n\n<p style=\"font-size: 1.1em; color: #444;\">\nWhen I looked at the public notebook scoring <strong>16</strong>, I realized it could be improved by carefully optimizing the inference. Here's the changes I made to the public notebook and whyy  : </p>\n\n<h2 style=\"color: #34495e;\">1. <strong>dtype and max_num_steps</strong></h2>\n<p style=\"color: #444;\">\nBefore the QwQ phase, I had a score of <strong>8</strong>. I noticed that setting <code>dtype = bfloat16</code> and increasing <code>max_num_seqs</code> improved the score and made it more stable.  \n</p>\n<blockquote style=\"background: #f4f4f9; padding: 10px; border-left: 4px solid #a3a3a3; margin: 10px 0;\">\n<strong>Settings:</strong>  \n<code>dtype = bfloat16</code>, <code>max_num_seqs = 256 (default)</code>\n</blockquote>\n\n<h2 style=\"color: #34495e;\">2. <strong>Change in Prompts</strong></h2>\n<p style=\"color: #444;\">\nSince QwQ leverages Chain-of-Thought (CoT) better than code generation, I decided to retain only the CoT prompts and remove those mentioning Python code generation.  \nHere are the 5 prompts I used for each sample:\n</p>\n<blockquote style=\"background: #f4f4f9; padding: 10px; border-left: 4px solid #a3a3a3; margin: 10px 0;\">\n<ul>\n  <li>Please use chained reasoning to put the answer in <code>\\\\boxed{}</code>.</li>\n  <li>Please reflect and verify while reasoning and put the answer in <code>\\\\boxed{}</code>.</li>\n  <li>Solve the following problem using concise and clear reasoning by placing the answer in <code>\\\\boxed{}</code>.</li>\n  <li>You are a helpful and reflective maths assistant, please reason step by step to put the answer in <code>\\\\boxed{}</code>.</li>\n  <li>You are the smartest maths expert in the world, please spike this question and put the answer in <code>\\\\boxed{}</code>.</li>\n</ul>\n</blockquote>\n\n<h2 style=\"color: #34495e;\">3. <strong>Number of Samples: 8 âž” 5</strong></h2>\n<p style=\"color: #444;\">\nFrom my local runs and discussions, I observed that QwQ mostly provided the correct answer with fewer samples (4â€“5). Using 8 samples was inefficient, as it increased runtime and risked exceeding the 5-hour limit before solving all 50 problems (solving 40-45 in total). Therefore, I reduced the number of samples to 5.\n</p>\n\n<h2 style=\"color: #34495e;\">4. <strong>Seed</strong></h2>\n<p style=\"color: #444;\">\nI removed the seed parameter from <code>vllm</code> and instead set a global seed using <code>set_seed(42)</code> from <code>transformers</code>.\n</p>\n\n<h2 style=\"color: #34495e;\">5. <strong>Got Lucky ðŸ˜‚</strong></h2>\n<p style=\"color: #444;\">\nSometimes, luck plays its part!\n</p>\n","metadata":{}},{"cell_type":"markdown","source":"**References of previous notebooks**\n\n- https://www.kaggle.com/code/richolson/ai-math-olympiad-qwen2-5-72b for showing how to submit\n- https://www.kaggle.com/code/abdullahmeda/load-72b-awq-model-using-vllm-on-l4-x4\n- https://www.kaggle.com/code/huikang/qwen2-5-math-1-5b-instruct\n- https://www.kaggle.com/code/boristown/qwen-qwq-32b-preview-deepreasoning","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"from transformers import set_seed\nset_seed(42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport warnings\n\nimport pandas as pd\nimport polars as pl\n\nimport torch\nimport kaggle_evaluation.aimo_2_inference_server\n\npd.set_option('display.max_colwidth', None)\ncutoff_time = time.time() + (4 * 60 + 45) * 60","metadata":{"papermill":{"duration":15.076705,"end_time":"2024-10-27T05:59:33.712437","exception":false,"start_time":"2024-10-27T05:59:18.635732","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T16:25:42.202223Z","iopub.execute_input":"2024-10-29T16:25:42.202621Z","iopub.status.idle":"2024-10-29T16:26:03.225944Z","shell.execute_reply.started":"2024-10-29T16:25:42.20258Z","shell.execute_reply":"2024-10-29T16:26:03.225028Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"from vllm import LLM, SamplingParams\n\nwarnings.simplefilter('ignore')\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndef clean_memory(deep=False):\n    gc.collect()\n    if deep:\n        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n    torch.cuda.empty_cache()\n\nllm_model_pth = '/kaggle/input/m/shelterw/qwen2.5/transformers/qwq-32b-preview-awq/1'\n\nllm = LLM(\n    llm_model_pth,\n    #dtype=\"half\",                -> Changed this\n    #max_num_seqs=128,            -> Changed this\n    max_model_len=32768,#4096*10,         \n    trust_remote_code=True,     \n    tensor_parallel_size=4,      \n    gpu_memory_utilization=0.96, \n)","metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = llm.get_tokenizer()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Utlities","metadata":{}},{"cell_type":"code","source":"import re\n\n#prompts\nthoughts = [\n    'Please use chained reasoning and put the answer in \\\\boxed{}.',\n    'Please reflect and verify while reasoning and put the answer in \\\\boxed{}.',\n    'Solve the following problem using concise and clear reasoning and put the answer in \\\\boxed{}.',\n    'Please reason carefully, with the help of a python programme, and finally put the answer into \\\\boxed{}.',\n    'You are a helpful and reflective math assistant, please reason step by step concisely to put the answer in \\\\boxed{}.',\n    # 'Solve the problem through concise and clear logical reasoning, and put the answer in \\\\boxed{}.',\n    'You are the smartest math expert in the world, please spike this problem and put the answer in \\\\boxed{}.'\n]\n\n#create single prompt\ndef make_next_prompt(text,round_idx):\n    default_prompt = thoughts[(round_idx+1)%len(thoughts)]\n    default_python_code = f\"print('{default_prompt}')\"\n    return default_python_code\n\n#extract python code from response\ndef extract_python_code(text):\n    pattern = r'```python\\s*(.*?)\\s*```'\n    matches = re.findall(pattern, text, re.DOTALL)\n    if matches:\n        ans = \"\\n\\n\".join(matches)\n        #print(f'Extracted python code: {ans}')\n        return ans\n    return \"\"\n\n#extract all code segments\ndef extract_python_code_list(text):\n    pattern = r'```python\\s*(.*?)\\s*```'\n    ans=[]\n    matches = re.findall(pattern, text, re.DOTALL)\n    for m in matches:\n        ans.append(m)\n    return ans\n\n#process the code\ndef process_python_code(query):\n    query = \"import math\\nimport numpy as np\\nimport sympy as sp\\n\" + query\n    current_rows = query.strip().split(\"\\n\")\n    new_rows = []\n    for row in current_rows:\n        new_rows.append(row)\n    ans = \"\\n\".join(new_rows)\n    print(f'Processed python code: {ans}')\n    return ans\n\nimport re\n\n#extract the answer from the boxes\ndef extract_boxed_texts(text):\n    pattern = r'oxed{(.*?)}'\n    matches = re.findall(pattern, text)\n    if not matches:\n        return []\n    ans = []\n    for content in matches:\n        if content.isdigit():\n            num = int(content)\n        else:\n            nums = re.findall(r'\\d+', content)\n            if not nums:\n                continue\n            num = int(nums[-1])\n        ans.append(num % 1000)\n    return ans\n    \n#extract the integer answer modulo 1000 from the boxes\ndef extract_boxed_text(text):\n    pattern = r'oxed{(.*?)}'\n    matches = re.findall(pattern, text)\n    if not matches:\n        return -1\n    content = matches[0]\n    if content.isdigit():\n        num = int(content)\n    else:\n        nums = re.findall(r'\\d+', content)\n        if not nums:\n            return -1\n        num = int(nums[-1])\n    return num % 1000\n\n#select the final answer based on the frequency/ majoity voting\nfrom collections import Counter\ndef select_answer(answers):\n    valid_answers = []\n    for answer in answers:\n        try:\n            if int(answer) == float(answer):\n                if 1 < int(answer) < 999 and int(answer) % 100 > 0:\n                    valid_answers.append(int(answer))\n        except:\n            pass\n    if not valid_answers:\n        return 49\n    _, answer = sorted([(v,k) for k,v in Counter(valid_answers).items()], reverse=True)[0]\n    return answer%1000","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport tempfile\nimport subprocess\n\n#Python REPL to execute code. taken from NuminaMath Solution\nclass PythonREPL:\n    def __init__(self, timeout=8):\n        self.timeout = timeout\n\n    def __call__(self, query):\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_file_path = os.path.join(temp_dir, \"tmp.py\")\n            with open(temp_file_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(query)\n            \n            try:\n                result = subprocess.run(\n                    [\"python3\", temp_file_path],\n                    capture_output=True,\n                    check=False,\n                    text=True,\n                    timeout=self.timeout,\n                )\n            except subprocess.TimeoutExpired:\n                return False, f\"Execution timed out after {self.timeout} seconds.\"\n\n            stdout = result.stdout.strip()\n            stderr = result.stderr.strip()\n\n            if result.returncode == 0:\n                return True, stdout\n            else:\n                # Process the error message to remove the temporary file path\n                # This makes the error message cleaner and more user-friendly\n                error_lines = stderr.split(\"\\n\")\n                cleaned_errors = []\n                for line in error_lines:\n                    if temp_file_path in line:\n                        # Remove the path from the error line\n                        line = line.replace(temp_file_path, \"<temporary_file>\")\n                    cleaned_errors.append(line)\n                cleaned_error_msg = \"\\n\".join(cleaned_errors)\n                # Include stdout in the error case\n                combined_output = f\"{stdout}\\n{cleaned_error_msg}\" if stdout else cleaned_error_msg\n                return False, combined_output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#sanity check\nlist_of_texts = [\n    tokenizer.apply_chat_template(\n        messages,\n        tokenize=True,\n        add_generation_prompt=True\n    )\n    for messages in [[{\"role\": \"user\", \"content\": \"hi\"}]]\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #define the sampling parameters\n# sampling_params = SamplingParams(\n#     temperature=1.0,              # Controls randomness in generation: higher values (e.g., 1.0) produce more diverse output.\n#     min_p=0.01,                   # Minimum cumulative probability for nucleus sampling, filtering out unlikely tokens.\n#     skip_special_tokens=True,     \n#     # max_tokens=1800,            \n#     max_tokens=32768,             # Sets a very high limit for token generation to handle longer outputs.\n#     # stop=[\"```output\"],       \n# )\n\nsampling_params = SamplingParams(\n    temperature=0.01,\n    repetition_penalty=1.0,\n    top_k=20,\n    top_p=0.8,\n    max_tokens=32768,\n)\n\n#generate prompts in batch\ndef batch_message_generate(list_of_messages) -> list[list[dict]]:\n    list_of_texts = [\n        tokenizer.apply_chat_template(\n            conversation=messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        for messages in list_of_messages\n    ]\n    \n    request_output = llm.generate(\n        prompts=list_of_texts,\n        sampling_params=sampling_params,\n    )\n    \n    for messages, single_request_output in zip(list_of_messages, request_output):\n        messages.append({'role': 'assistant', 'content': single_request_output.outputs[0].text})\n        print(messages[-1])\n\n    return list_of_messages","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#filter answers from the responses\n\ndef batch_message_filter(list_of_messages,list_of_idx) -> tuple[list[list[dict]], list[str]]:\n    global answer_contributions\n    extracted_answers = []\n    list_of_messages_to_keep = []\n    list_of_idx_to_keep = []\n    for idx,messages in zip(list_of_idx,list_of_messages):\n        answers = extract_boxed_texts(messages[-1]['content'])\n        if answers:\n            extracted_answers.extend(answers)\n            for answer in answers:\n                answer_contributions[answer].append(idx)\n        else:\n            list_of_messages_to_keep.append(messages)\n            list_of_idx_to_keep.append(idx)\n    return list_of_messages_to_keep, extracted_answers, list_of_idx_to_keep","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#execute the codes in the responses\ndef batch_message_execute(list_of_messages,round_idx) -> list[list[dict]]:\n    for messages in list_of_messages:\n        python_code = extract_python_code(messages[-1]['content'],round_idx)\n        python_code = process_python_code(python_code)\n        try:\n            success, output = PythonREPL()(python_code)\n        except Exception as e:\n            output = str(e)\n        messages.append({'role': 'user', 'content': output})\n        print(messages[-1])\n    return list_of_messages\n\n#execute the code and generate the answer from responses\ndef batch_message_execute_and_get_answer(list_of_messages,round_idx) -> tuple[list[list[dict]], list[int]]:\n    ans = []\n    for messages in list_of_messages:\n        python_code = extract_python_code(messages[-1]['content'])\n        python_code = process_python_code(python_code)\n        try:\n            success, output = PythonREPL()(python_code)\n            if success:\n                patten = r'(\\d+)'\n                matches = re.findall(patten, output)\n                if matches:\n                    for match in matches:\n                        ans.append(int(match)%1000)\n                        ans.append(int(match)%1000) #ä»£ç æƒé‡é«˜äºŽè‡ªç„¶è¯­è¨€ï¼Œæ‰€ä»¥æ·»åŠ ä¸¤æ¬¡ \n        except Exception as e:\n            output = str(e)\n        print(f'python code output: {output}')\n    return ans\n\n#execute code and generate answer for all elements in batch\ndef batch_message_list_execute_and_get_answer(list_of_messages,round_idx) -> tuple[list[list[dict]], list[int]]:\n    ans = []\n    for messages in list_of_messages:\n        python_code_list = extract_python_code_list(messages[-1]['content'])\n        for python_code in python_code_list:\n            python_code = process_python_code(python_code)\n            print(\"\\npython code: \\n\", python_code)\n            try:\n                success, output = PythonREPL()(python_code)\n                if success:\n                    patten = r'(\\d+)'\n                    matches = re.findall(patten, output)\n                    if matches:\n                        for match in matches:\n                            ans.append(int(match)%1000)\n                            ans.append(int(match)%1000) \n            except Exception as e:\n                output = str(e)\n            print(f'\\npython code output: {output}\\n')\n    return ans","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nimport pandas as pd\nimport polars as pl\n\n#API for competition submission\nimport kaggle_evaluation.aimo_2_inference_server","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#correct answers for reference problems\ndef get_correct_answer(question):\n    if 'Three airline' in question: return 79\n    if 'Fred and George' in question: return 250\n    if 'Triangle $ABC$' in question: return 180\n    if 'Find the three' in question: return 143\n    if 'We call a' in question: return 3\n    if 'Let $ABC$ be' in question: return 751\n    if 'For a positive' in question: return 891\n    if 'For positive integers' in question: return 810\n    if 'The Fibonacci numbers' in question: return 201\n    if 'Alice writes all' in question: return 902\n    return 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"#predict function to solve single problem\n\nfrom collections import Counter, defaultdict\ng_score = 0\ng_count = 0\nprompt_score = Counter()\nanswer_contributions = defaultdict(list)\ndef predict_for_question(question: str) -> int:\n    global g_score\n    global g_count\n    global prompt_score\n    global answer_contributions\n    question += \"\\nIf the final answer is a number larger than 1000, take modulo 1000. \"\n    if time.time() > cutoff_time: \n        return 210\n    print(question)\n    \n    list_of_messages = [\n        [\n            {\"role\": \"system\", \"content\": thoughts[k]},\n            {\"role\": \"user\", \"content\": question}\n        ] for k in range(6)\n    ]\n\n    all_extracted_answers = []\n    list_of_idx = list(range(len(list_of_messages)))\n    max_round = 1\n    for round_idx in range(max_round):\n        print(f\"round {round_idx+1}\")\n        list_of_messages = batch_message_generate(list_of_messages)\n        #extracted_python_answer = batch_message_execute_and_get_answer(list_of_messages,round_idx)\n        extracted_python_answer = batch_message_list_execute_and_get_answer(list_of_messages,round_idx)\n        list_of_messages, extracted_answers, list_of_idx  = batch_message_filter(list_of_messages, list_of_idx)\n        all_extracted_answers.extend(extracted_python_answer)\n        all_extracted_answers.extend(extracted_answers)\n        print(\"extracted boxed answers:\",extracted_answers)\n        print(\"extracted python answers:\",extracted_python_answer)\n        print(\"all extracted answers:\",all_extracted_answers)\n        if not list_of_messages:\n            break\n        #list_of_messages = batch_message_execute(list_of_messages,round_idx)\n    answer = select_answer(all_extracted_answers)\n    print(\"answer:\",answer)\n    correct_answer = get_correct_answer(question)\n    print(\"correct answer:\",correct_answer)\n    g_count += 1\n    if str(answer) == str(correct_answer):\n        g_score += 1\n\n    print(f\"score: {g_score}/{g_count}\")\n    print(\"\\n\\n\")\n    return answer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Replace this function with your inference code.\n# The function should return a single integer between 0 and 999, inclusive.\n# Each prediction (except the very first) must be returned within 30 minutes of the question being provided.\ndef predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n    id_ = id_.item(0)\n    print(\"------\")\n    print(id_)\n    \n    question = question.item(0)\n    answer = predict_for_question(question)\n    print(question)\n    print(\"------\\n\\n\\n\")\n    return pl.DataFrame({'id': id_, 'answer': answer})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.read_csv(\n    '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv'\n).drop('answer', axis=1).to_csv('reference.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        (\n            'reference.csv',\n            # '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/test.csv',\n        )\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}